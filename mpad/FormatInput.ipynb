{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Create Train/Test files. (Only Train documents will need to be augmented).\\n2. Generate and save train augmentations. \\n3. We need to create a SHARED vocab over train, val, test sets. \\n4. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Create Train/Test files. (Only Train documents will need to be augmented).\n",
    "2. Generate and save train augmentations. \n",
    "3. We need to create a SHARED vocab over train, val, test sets. \n",
    "4. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"R8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_name_list = []\n",
    "with open('../data/' + dataset + '.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_name_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "R8_label_dict = {'acq':0, 'crude':1, 'earn':2, 'grain':3, 'interest':4, 'money-fx':5, 'ship':6, 'trade':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples:  5485\n",
      "Train Labels:  5485\n",
      "Test Samples:  2189\n",
      "Test Samples:  2189\n"
     ]
    }
   ],
   "source": [
    "# load raw text\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "with open('../data/corpus/' + dataset + '.clean.txt', 'r') as f:\n",
    "    for name, line in zip(doc_name_list,f.readlines()):\n",
    "        idx, split, label = name.split(\"\\t\")\n",
    "        if split == 'train':\n",
    "            doc_train_list.append(\"{}\\t{}\\n\".format(idx,line.strip()))\n",
    "            label = R8_label_dict[label]\n",
    "            train_labels.append(label)\n",
    "        elif split == 'test':\n",
    "            doc_test_list.append(\"{}\\t{}\\n\".format(idx,line.strip()))\n",
    "            label = R8_label_dict[label]\n",
    "            test_labels.append(label)\n",
    "print(\"Train Samples: \", len(doc_train_list))\n",
    "print(\"Train Labels: \",len(train_labels))\n",
    "print(\"Test Samples: \", len(doc_test_list))\n",
    "print(\"Test Samples: \",len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists!\n",
      "Train saved!\n",
      "Test saved!\n"
     ]
    }
   ],
   "source": [
    "# save into parsed train vs. test files\n",
    "try: \n",
    "    os.makedirs(\"../aug_data/{}\".format(dataset))\n",
    "except:\n",
    "    print(\"Directory exists!\")\n",
    "    \n",
    "with open(\"../aug_data/{}_train.txt\".format(dataset),'w') as file:\n",
    "    for l in doc_train_list:\n",
    "        file.writelines(l)\n",
    "\n",
    "with open(\"../aug_data/{}_train_labels.txt\".format(dataset),'w') as file:\n",
    "    for l in train_labels:\n",
    "        file.writelines(str(l))\n",
    "print(\"Train saved!\")\n",
    "\n",
    "with open(\"../aug_data/{}_test.txt\".format(dataset),'w') as file:\n",
    "    for l in doc_test_list:\n",
    "        file.writelines(l)\n",
    "        \n",
    "with open(\"../aug_data/{}_test_labels.txt\".format(dataset),'w') as file:\n",
    "    for l in test_labels:\n",
    "        file.writelines(str(l))\n",
    "print(\"Test saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n5 ../aug_data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate + Clean Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command: \n",
    "python ../../eda_nlp/code/augment.py --input=[dataset_train.txt] --output=[dataset_train_aug.txt] --num_aug=[x2epochs] --alpha_sr=0.05 --alpha_rd=0.1 --alpha_ri=0.05 --alpha_rs=0.05\n",
    "python remove_words.py --filename=[dataset_train_aug.txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n5 ../aug_data/mr/mr_train_aug_clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Augmentation File into Epoch Files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the augmented file given label ids. \n",
    "\"\"\"\n",
    "\n",
    "file_name = \"../aug_data/R8/R8_train_aug_clean_85.txt\"\n",
    "dataset= \"R8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists!\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    os.makedirs(\"../aug_data/{}/raw\".format(dataset))\n",
    "except:\n",
    "    print(\"Directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dict = collections.defaultdict(list)\n",
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        idx = line.split(\" \")[0]\n",
    "        l = \" \".join(line.split(\" \")[1:])\n",
    "        #idx,l = line.strip().split(\"\\t\")\n",
    "        aug_dict[int(idx)].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create dictionary of sample ids, and augmented samples\n",
    "## i.e. aug_dict[0] = [sample_0, sample_0_aug_1, sample_0_aug_2, ...]\n",
    "aug_dict = collections.defaultdict(list)\n",
    "skip_count = 0\n",
    "skip_list = []\n",
    "temp_vals = []\n",
    "with open(file_name, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        idx = line.split(\" \")[0]\n",
    "        l = \" \".join(line.split(\" \")[1:])\n",
    "        \n",
    "        #if there is a dead sample, replace with the one before it!\n",
    "        if len(l) == 0:\n",
    "            print(\"Found a skipped item: \",idx)\n",
    "            try:\n",
    "                l = aug_dict[int(idx)][-1]\n",
    "            except:\n",
    "                l = \"TEMP\"\n",
    "                print(\"TEMP ASSIGNED AT: \",idx)\n",
    "                temp_vals.append(idx)\n",
    "            skip_count += 1\n",
    "            skip_list.append(int(idx.split()[0]))\n",
    "        #idx,l = line.strip().split(\"\\t\")\n",
    "        aug_dict[int(idx)].append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,    0.,    0.,    0.,    0., 5485.,    0.,    0.,    0.,\n",
       "           0.]),\n",
       " array([85.5, 85.6, 85.7, 85.8, 85.9, 86. , 86.1, 86.2, 86.3, 86.4, 86.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = []\n",
    "for k,v in aug_dict.items():\n",
    "    counts.append(len(v))\n",
    "plt.hist(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug Dict Keys:  5485\n",
      "Aug Samples per Key:  86\n",
      "Sample 0:  ['champion products approves stock split champion products inc said circuit board directors approved two one stock split common shares shareholders record apr company also said circuit board voted recommend shareholders annual meeting apr increase authorized capital stock five mln mln shares reuter\\n', 'champion products increment approves stock split champion products inc said board directors approved two one stock split common shares shareholders record april company also said board voted recommend shareholders annual music director meeting april increase authorized capital stock five mln mln shares reuter\\n', 'champion products approves stock split directors products inc said board champion approved two one stock split common voted shareholders record april company also said board shares recommend shareholders annual meeting april increase authorized capital stock five mln mln shares reuter\\n']\n"
     ]
    }
   ],
   "source": [
    "print(\"Aug Dict Keys: \",len(aug_dict.keys()))\n",
    "print(\"Aug Samples per Key: \",len(aug_dict[0]))\n",
    "print(\"Sample 0: \",aug_dict[0][0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processed:  0\n",
      "=> Processed:  1\n",
      "=> Processed:  2\n",
      "=> Processed:  3\n",
      "=> Processed:  4\n",
      "=> Processed:  5\n",
      "=> Processed:  6\n",
      "=> Processed:  7\n",
      "=> Processed:  8\n",
      "=> Processed:  9\n",
      "=> Processed:  10\n",
      "=> Processed:  11\n",
      "=> Processed:  12\n",
      "=> Processed:  13\n",
      "=> Processed:  14\n",
      "=> Processed:  15\n",
      "=> Processed:  16\n",
      "=> Processed:  17\n",
      "=> Processed:  18\n",
      "=> Processed:  19\n",
      "=> Processed:  20\n",
      "=> Processed:  21\n",
      "=> Processed:  22\n",
      "=> Processed:  23\n",
      "=> Processed:  24\n",
      "=> Processed:  25\n",
      "=> Processed:  26\n",
      "=> Processed:  27\n",
      "=> Processed:  28\n",
      "=> Processed:  29\n",
      "=> Processed:  30\n",
      "=> Processed:  31\n",
      "=> Processed:  32\n",
      "=> Processed:  33\n",
      "=> Processed:  34\n",
      "=> Processed:  35\n",
      "=> Processed:  36\n",
      "=> Processed:  37\n",
      "=> Processed:  38\n",
      "=> Processed:  39\n",
      "=> Processed:  40\n",
      "=> Processed:  41\n",
      "=> Processed:  42\n",
      "=> Processed:  43\n",
      "=> Processed:  44\n",
      "=> Processed:  45\n",
      "=> Processed:  46\n",
      "=> Processed:  47\n",
      "=> Processed:  48\n",
      "=> Processed:  49\n",
      "=> Processed:  50\n",
      "=> Processed:  51\n",
      "=> Processed:  52\n",
      "=> Processed:  53\n",
      "=> Processed:  54\n",
      "=> Processed:  55\n",
      "=> Processed:  56\n",
      "=> Processed:  57\n",
      "=> Processed:  58\n",
      "=> Processed:  59\n",
      "=> Processed:  60\n",
      "=> Processed:  61\n",
      "=> Processed:  62\n",
      "=> Processed:  63\n",
      "=> Processed:  64\n",
      "=> Processed:  65\n",
      "=> Processed:  66\n",
      "=> Processed:  67\n",
      "=> Processed:  68\n",
      "=> Processed:  69\n",
      "=> Processed:  70\n",
      "=> Processed:  71\n",
      "=> Processed:  72\n",
      "=> Processed:  73\n",
      "=> Processed:  74\n",
      "=> Processed:  75\n",
      "=> Processed:  76\n",
      "=> Processed:  77\n",
      "=> Processed:  78\n",
      "=> Processed:  79\n",
      "=> Processed:  80\n",
      "=> Processed:  81\n"
     ]
    }
   ],
   "source": [
    "#for num augmentations per sample. Make a separate pickle.\n",
    "for i in range(0,82):\n",
    "    with open(\"../aug_data/{0}/raw/{0}_{1}.txt\".format(dataset,i), 'w') as file:\n",
    "        for k,v in aug_dict.items():\n",
    "            file.writelines(v[i])\n",
    "    print(\"=> Processed: \",i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "champion products approves stock split champion products inc said circuit board directors approved two one stock split common shares shareholders record apr company also said circuit board voted recommend shareholders annual meeting apr increase authorized capital stock five mln mln shares reuter\r\n",
      "computer systems completes sale computer terminal systems inc said completed sale shares common stock warrants additional one mln shares n v dlrs company said warrants exercisable five years purchase price dlrs per share computer terminal said also right buy additional shares increase total holdings pct terminal outstanding common stock certain circumstances control company company occur warrants would exercisable price pct common stock market price exceed dlrs per share computer also said sold dot impact technology including future improvements inc houston tex dlrs said would continue exclusive licensee technology company said moves part reorganization plan would pay current operation ensure product delivery computer terminal makes computer forms ticket printers reuter\r\n",
      "inc year net shr cts vs dlrs net vs assets mln vs mln deposits mln vs mln loans mln vs mln note th qtr available year includes extraordinary gain tax carry forward dlrs five cts per shr reuter\r\n",
      "international inc nd qtr oper shr loss two cts seven cts oper shr profit vs profit revs mln vs mln shrs mln vs six mths oper shr profit nil vs profit cts oper net profit vs profit revs mln vs mln avg shrs mln vs mln per shr calculated payment preferred dividends results exclude credits four cts nine cts qtr six mths vs six cts cts prior periods operating carryforwards\r\n",
      "brown forman inc th qtr net shr one dlr vs cts net mln vs mln revs mln vs mln nine mths dlrs vs net mln vs mln billion vs mln reuter\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 ../aug_data/R8/raw/R8_0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5 ../data/corpus/mr.clean.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an augmented version of each training/validation sentence for a fixed number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save Vocabulary over Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to create a joint corpus. We can directly use the AUGMENTED CLEAN file. We must also include the TEST file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= 'R8'\n",
    "window_size = 3\n",
    "weighted_graph = False\n",
    "truncate = False # whether to truncate long document\n",
    "MAX_TRUNC_LEN = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings\n",
    "word_embeddings_dim = 300\n",
    "word_embeddings = {}\n",
    "\n",
    "with open('../glove.6B.' + str(word_embeddings_dim) + 'd.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        data = line.split()\n",
    "        word_embeddings[str(data[0])] = list(map(float,data[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_content_list = []\n",
    "\n",
    "#load AUG TRAIN text\n",
    "with open('../aug_data/{0}/{0}_train.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "# load CLEAN TRAIN text\n",
    "with open('../aug_data/{0}/{0}_train_aug_clean_85.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_content_list.append(line.strip())\n",
    "\n",
    "#load TEST text\n",
    "with open('../aug_data/{0}/{0}_test.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_content_list.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus vocabulary OVER ALL DATA\n",
    "word_set = set()\n",
    "\n",
    "for doc_words in doc_content_list:\n",
    "    words = doc_words.split()\n",
    "    word_set.update(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize out-of-vocabulary word embeddings\n",
    "oov = {}\n",
    "for v in vocab:\n",
    "    oov[v] = np.random.uniform(-0.01, 0.01, word_embeddings_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the word map and oov word embeddings \n",
    "with open(\"../aug_data/{0}/raw/{0}_word_map.pkl\".format(dataset), 'wb') as output_file:\n",
    "    pkl.dump(word_id_map,output_file)\n",
    "\n",
    "with open(\"../aug_data/{0}/raw/{0}_oov.pkl\".format(dataset), 'wb') as output_file:\n",
    "    pkl.dump(oov,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the word map and oov word embeddings \n",
    "# with open(\"../aug_data/raw/word_map.pkl\", 'rb') as input_file:\n",
    "#     word_id_map = pkl.load(input_file)\n",
    "\n",
    "# with open(\"../aug_data/raw/oov.pkl\", 'rb') as input_file:\n",
    "#     oov = pkl.load(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build label list\n",
    "doc_names = []\n",
    "with open('../data/{}.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_names.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R8'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Labels:  5485\n"
     ]
    }
   ],
   "source": [
    "labels = np.zeros((5485,8))\n",
    "train_count = 0\n",
    "test_count = 0\n",
    "train_labels = []\n",
    "for enum, doc_meta in enumerate(doc_names):\n",
    "    temp = doc_meta.split('\\t')\n",
    "    if temp[1] == 'train': \n",
    "        idx = R8_label_dict[temp[2]]\n",
    "        train_labels.append(label)\n",
    "        labels[train_count][idx] = 1\n",
    "        train_count += 1\n",
    "    else:\n",
    "        test_count += 1\n",
    "print('Train Labels: ',len(labels))\n",
    "np.save(\"../aug_data/{0}/{0}_train_y.npy\".format(dataset), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Labels:  2189\n"
     ]
    }
   ],
   "source": [
    "test_labels = np.zeros((2189,8))\n",
    "test_count = 0\n",
    "for enum, doc_meta in enumerate(doc_names):\n",
    "    temp = doc_meta.split('\\t')\n",
    "    if temp[1] == 'test': \n",
    "        idx = R8_label_dict[temp[2]]\n",
    "        test_labels[test_count][idx] = 1\n",
    "        test_count += 1\n",
    "    else:\n",
    "        pass\n",
    "print('Test Labels: ',len(test_labels))\n",
    "np.save(\"../aug_data/{0}/{0}_test_y.npy\".format(dataset), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5485, 2189)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count, test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graph function\n",
    "def build_graph(start, end,doc_content_sublist):\n",
    "    x_adj = []\n",
    "    x_feature = []\n",
    "    y = []\n",
    "    doc_len_list = []\n",
    "    vocab_set = set()\n",
    "\n",
    "    for i in tqdm(range(start, end),disable=True):\n",
    "        doc_words = doc_content_sublist[i].split()\n",
    "        if truncate:\n",
    "            doc_words = doc_words[:MAX_TRUNC_LEN]\n",
    "        doc_len = len(doc_words)\n",
    "\n",
    "        doc_vocab = list(set(doc_words))\n",
    "        doc_nodes = len(doc_vocab)\n",
    "\n",
    "        doc_len_list.append(doc_nodes)\n",
    "        vocab_set.update(doc_vocab)\n",
    "\n",
    "        doc_word_id_map = {}\n",
    "        for j in range(doc_nodes):\n",
    "            doc_word_id_map[doc_vocab[j]] = j\n",
    "\n",
    "        # sliding windows\n",
    "        windows = []\n",
    "        if doc_len <= window_size:\n",
    "            windows.append(doc_words)\n",
    "        else:\n",
    "            for j in range(doc_len - window_size + 1):\n",
    "                window = doc_words[j: j + window_size]\n",
    "                windows.append(window)\n",
    "\n",
    "        word_pair_count = {}\n",
    "        for window in windows:\n",
    "            for p in range(1, len(window)):\n",
    "                for q in range(0, p):\n",
    "                    word_p = window[p]\n",
    "                    word_p_id = word_id_map[word_p]\n",
    "                    word_q = window[q]\n",
    "                    word_q_id = word_id_map[word_q]\n",
    "                    if word_p_id == word_q_id:\n",
    "                        continue\n",
    "                    word_pair_key = (word_p_id, word_q_id)\n",
    "                    # word co-occurrences as weights\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key] += 1.\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key] = 1.\n",
    "                    # bi-direction\n",
    "                    word_pair_key = (word_q_id, word_p_id)\n",
    "                    if word_pair_key in word_pair_count:\n",
    "                        word_pair_count[word_pair_key] += 1.\n",
    "                    else:\n",
    "                        word_pair_count[word_pair_key] = 1.\n",
    "    \n",
    "        row = []\n",
    "        col = []\n",
    "        weight = []\n",
    "        features = []\n",
    "\n",
    "        for key in word_pair_count:\n",
    "            p = key[0]\n",
    "            q = key[1]\n",
    "            row.append(doc_word_id_map[vocab[p]])\n",
    "            col.append(doc_word_id_map[vocab[q]])\n",
    "            weight.append(word_pair_count[key] if weighted_graph else 1.)\n",
    "        adj = sp.csr_matrix((weight, (row, col)), shape=(doc_nodes, doc_nodes))\n",
    "    \n",
    "        for k, v in sorted(doc_word_id_map.items(), key=lambda x: x[1]):\n",
    "            features.append(word_embeddings[k] if k in word_embeddings else oov[k])\n",
    "\n",
    "        x_adj.append(adj)\n",
    "        x_feature.append(features)\n",
    "    \n",
    "    return x_adj, x_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    os.makedirs(\"../aug_data/{}/processed\".format(dataset))\n",
    "except:\n",
    "    print(\"Directory exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processed:  0 5485\n",
      "=> Processed:  1 5485\n",
      "=> Processed:  2 5485\n",
      "=> Processed:  3 5485\n",
      "=> Processed:  4 5485\n",
      "=> Processed:  5 5485\n",
      "=> Processed:  6 5485\n",
      "=> Processed:  7 5485\n",
      "=> Processed:  8 5485\n",
      "=> Processed:  9 5485\n",
      "=> Processed:  10 5485\n",
      "=> Processed:  11 5485\n",
      "=> Processed:  12 5485\n",
      "=> Processed:  13 5485\n",
      "=> Processed:  14 5485\n",
      "=> Processed:  15 5485\n",
      "=> Processed:  16 5485\n",
      "=> Processed:  17 5485\n",
      "=> Processed:  18 5485\n",
      "=> Processed:  19 5485\n",
      "=> Processed:  20 5485\n",
      "=> Processed:  21 5485\n",
      "=> Processed:  22 5485\n",
      "=> Processed:  23 5485\n",
      "=> Processed:  24 5485\n",
      "=> Processed:  25 5485\n",
      "=> Processed:  26 5485\n",
      "=> Processed:  27 5485\n",
      "=> Processed:  28 5485\n",
      "=> Processed:  29 5485\n",
      "=> Processed:  30 5485\n",
      "=> Processed:  31 5485\n",
      "=> Processed:  32 5485\n",
      "=> Processed:  33 5485\n",
      "=> Processed:  34 5485\n",
      "=> Processed:  35 5485\n",
      "=> Processed:  36 5485\n",
      "=> Processed:  37 5485\n",
      "=> Processed:  38 5485\n",
      "=> Processed:  39 5485\n",
      "=> Processed:  40 5485\n",
      "=> Processed:  41 5485\n",
      "=> Processed:  42 5485\n",
      "=> Processed:  43 5485\n",
      "=> Processed:  44 5485\n",
      "=> Processed:  45 5485\n",
      "=> Processed:  46 5485\n",
      "=> Processed:  47 5485\n",
      "=> Processed:  48 5485\n",
      "=> Processed:  49 5485\n",
      "=> Processed:  50 5485\n",
      "=> Processed:  51 5485\n",
      "=> Processed:  52 5485\n",
      "=> Processed:  53 5485\n",
      "=> Processed:  54 5485\n",
      "=> Processed:  55 5485\n",
      "=> Processed:  56 5485\n",
      "=> Processed:  57 5485\n",
      "=> Processed:  58 5485\n",
      "=> Processed:  59 5485\n",
      "=> Processed:  60 5485\n",
      "=> Processed:  61 5485\n",
      "=> Processed:  62 5485\n",
      "=> Processed:  63 5485\n",
      "=> Processed:  64 5485\n",
      "=> Processed:  65 5485\n",
      "=> Processed:  66 5485\n",
      "=> Processed:  67 5485\n",
      "=> Processed:  68 5485\n",
      "=> Processed:  69 5485\n",
      "=> Processed:  70 5485\n",
      "=> Processed:  71 5485\n",
      "=> Processed:  72 5485\n",
      "=> Processed:  73 5485\n",
      "=> Processed:  74 5485\n",
      "=> Processed:  75 5485\n",
      "=> Processed:  76 5485\n",
      "=> Processed:  77 5485\n",
      "=> Processed:  78 5485\n",
      "=> Processed:  79 5485\n",
      "=> Processed:  80 5485\n",
      "=> Processed:  81 5485\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../aug_data/R8/raw/R8_82.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-bc4e13787cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdoc_content_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../aug_data/{0}/raw/{0}_{1}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mdoc_content_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../aug_data/R8/raw/R8_82.txt'"
     ]
    }
   ],
   "source": [
    "## Create a graph pkl file for every augmented epoch. (the txt files generated above!)\n",
    "short_list = []\n",
    "for epoch in range(85):\n",
    "    doc_content_list = []\n",
    "    with open('../aug_data/{0}/raw/{0}_{1}.txt'.format(dataset,epoch), 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            doc_content_list.append(line.strip())\n",
    "    adj, features = build_graph(start=0,end=len(doc_content_list),doc_content_sublist=doc_content_list)\n",
    "    \n",
    "    with open(\"../aug_data/{0}/processed/{0}_adj_{1}.pkl\".format(dataset,epoch),'wb') as file:\n",
    "        pkl.dump(adj,file)\n",
    "    \n",
    "    with open(\"../aug_data/{0}/processed/{0}_feat_{1}.pkl\".format(dataset,epoch),'wb') as file:\n",
    "        pkl.dump(features,file)\n",
    "        \n",
    "    if len(doc_content_list) < 5485:\n",
    "        print(\"*\"*50)\n",
    "        print(\"MISSING SAMPLES!\", epoch)\n",
    "        print(\"*\"*50)\n",
    "        short_list.append(epoch)\n",
    "    print(\"=> Processed: \",epoch, len(doc_content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../aug_data/{0}/processed/{0}_adj_test.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(adj,file)\n",
    "    \n",
    "with open(\"../aug_data/{0}/processed/{0}_feat_test.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(features,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTANT: SAVE THE TEST SET USING THE PROVIDED VOCABULARY AND MAPPING!\n",
    "\"\"\"\n",
    "doc_content_list = []\n",
    "with open('../aug_data/{0}/{0}_test.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_content_list.append(line.split(\"\\t\")[1].strip())\n",
    "adj, features = build_graph(start=0,end=len(doc_content_list),doc_content_sublist=doc_content_list)\n",
    "\n",
    "\n",
    "with open(\"../aug_data/{0}/processed/{0}_adj_test.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(adj,file)\n",
    "    \n",
    "with open(\"../aug_data/{0}/processed/{0}_feat_test.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(features,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTANT: SAVE THE TRAIN SET USING THE PROVIDED VOCABULARY AND MAPPING!\n",
    "\"\"\"\n",
    "doc_content_list = []\n",
    "with open('../aug_data/{0}/{0}_train.txt'.format(dataset), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        doc_content_list.append(line.split(\"\\t\")[1].strip())\n",
    "adj, features = build_graph(start=0,end=len(doc_content_list),doc_content_sublist=doc_content_list)\n",
    "\n",
    "\n",
    "with open(\"../aug_data/{0}/processed/{0}_adj_train.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(adj,file)\n",
    "    \n",
    "with open(\"../aug_data/{0}/processed/{0}_feat_train.pkl\".format(dataset),'wb') as file:\n",
    "    pkl.dump(features,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
