{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils import load_file, preprocessing, get_vocab, load_embeddings, create_gows, accuracy, generate_batches, AverageMeter, train,train_simsiam, load_R8, dotdict, knn\n",
    "from models import MPAD, SimSiam\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['path_to_dataset'] = '../datasets/R8.txt'\n",
    "args['path_to_embeddings'] = \"../GoogleNews-vectors-negative300.bin\"\n",
    "args['no_cuda'] = False\n",
    "args['epochs'] = 200\n",
    "args['lr'] = 0.001\n",
    "args['hidden'] = 64\n",
    "args['penultimate'] = 64\n",
    "args['message_passing_layers']=2\n",
    "args['window_size'] = 2\n",
    "args['directed'] = True\n",
    "args['use_master_node'] = True\n",
    "args['normalize'] = True\n",
    "args['dropout'] = 0.5\n",
    "args['batch_size'] = 128\n",
    "args['patience'] = 20\n",
    "args['rand_node_drop'] = 0.3\n",
    "args['dataset'] = 'subjectivity'\n",
    "args['use_unique'] = True\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are saving a different file for every epoch of NLP augmentations => Therefore, we create a utility loader to process each of these files instead of loading up front. \n",
    "LabelEncoder, Vocab and Embeddings are shared across augmented files. We need to initialize the vocab and embeddings over the superset of augmented sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, _ = load_file(\"../datasets/subjectivity/unique/consolidated.txt\")\n",
    "#docs, _ = load_file(\"../datasets/subjectivity.txt\")\n",
    "docs = preprocessing(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  36104\n",
      "Existing vectors: 29329\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(docs)\n",
    "embeddings = load_embeddings(\"../GoogleNews-vectors-negative300.bin\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epoch_txt(epoch_num,l_enc,args):\n",
    "    file_name = \"../datasets/{dataset}/{unique}/{epoch_num}.txt\".format(dataset=args.dataset,unique =\"unique\" if args.use_unique else \"non_unique\",epoch_num = epoch_num)\n",
    "    docs, class_labels = load_file(file_name)\n",
    "    docs = preprocessing(docs)\n",
    "    class_labels = l_enc.fit_transform(class_labels)\n",
    "    nclass = np.unique(class_labels).size\n",
    "    y = list()\n",
    "    for i in range(len(class_labels)):\n",
    "        t = np.zeros(1)\n",
    "        t[0] = class_labels[i]\n",
    "        y.append(t)\n",
    "        \n",
    "    adj, features, _ = create_gows(docs, vocab, args.window_size, args.directed, args.normalize, args.use_master_node)\n",
    "    return adj,features,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 261 ms, total: 18.6 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adj,features,y = load_epoch_txt(epoch_num=0,l_enc=l_enc,args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training splits\n",
    "\n",
    "These splits will be shared over all epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "it = 0\n",
    "accs = list()\n",
    "train_index, test_index =  kf.split(y)\n",
    "train_index = train_index[0]\n",
    "test_index = test_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(train_index)\n",
    "train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "val_index = idx[int(idx.size*0.9):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  4497\n",
      "VAL:  500\n",
      "TEST:  4998\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_index)\n",
    "n_val = len(val_index)\n",
    "n_test = len(test_index)\n",
    "\n",
    "print(\"TRAIN: \",n_train)\n",
    "print(\"VAL: \",n_val)\n",
    "print(\"TEST: \",n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Val,Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, _ = load_epoch_txt(0,l_enc,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.3 s, sys: 200 ms, total: 11.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "adj_val = [adj[i] for i in val_index]\n",
    "features_val = [features[i] for i in val_index]\n",
    "y_val = [y[i] for i in val_index]\n",
    "adj_val, features_val, batch_n_graphs_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, args.use_master_node)\n",
    "\n",
    "adj_test = [adj[i] for i in test_index]\n",
    "features_test = [features[i] for i in test_index]\n",
    "y_test = [y[i] for i in test_index]\n",
    "adj_test, features_test, batch_n_graphs_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, args.use_master_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    adj_train = [x.cuda() for x in adj_train]\n",
    "    features_train = [x.cuda() for x in features_train]\n",
    "    batch_n_graphs_train = [x.cuda() for x in batch_n_graphs_train]\n",
    "    y_train = [x.cuda() for x in y_train]\n",
    "    adj_val = [x.cuda() for x in adj_val]\n",
    "    features_val = [x.cuda() for x in features_val]\n",
    "    batch_n_graphs_val = [x.cuda() for x in batch_n_graphs_val]\n",
    "    y_val = [x.cuda() for x in y_val]\n",
    "    adj_test = [x.cuda() for x in adj_test]\n",
    "    features_test = [x.cuda() for x in features_test]\n",
    "    batch_n_graphs_test = [x.cuda() for x in batch_n_graphs_test]\n",
    "    y_test = [x.cuda() for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 2\n",
    "model = MPAD(embeddings.shape[1], \n",
    "             args.message_passing_layers, \n",
    "             args.hidden,\n",
    "             args.penultimate, \n",
    "             n_class, \n",
    "             args.dropout,\n",
    "             embeddings, \n",
    "             args.use_master_node)\n",
    "\n",
    "model.embedding_dim = 64\n",
    "\n",
    "sim = SimSiam(backbone=model,project_dim=64,bottle_neck_dim=10)\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, sim.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "args['lr'] = 0.001\n",
    "optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8, eta_min=1e-5,verbose=True)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args['lr'] = 0.03\n",
    "# optimizer = optim.SGD(parameters, lr=args.lr)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8, eta_min=1e-5,verbose=True)\n",
    "# #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.epochs = 50\n",
    "n_train_batches = ceil(n_train/args.batch_size)\n",
    "n_val_batches = ceil(n_val/args.batch_size)\n",
    "n_test_batches = ceil(n_test/args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch Loss:  -0.38832876767430985\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "KNN Acc:  0.8395358143257303\n",
      "==================================================\n",
      "Epoch Loss:  -0.544455223424094\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "KNN Acc:  0.8457382953181273\n",
      "==================================================\n",
      "Epoch Loss:  -0.6798791902405875\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "KNN Acc:  0.8617446978791516\n",
      "==================================================\n",
      "Epoch Loss:  -0.7834587165287563\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "KNN Acc:  0.8625450180072028\n",
      "==================================================\n",
      "Epoch Loss:  -0.827249881199428\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "KNN Acc:  0.8641456582633054\n",
      "==================================================\n",
      "Epoch Loss:  -0.8456495642662049\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "KNN Acc:  0.8613445378151261\n",
      "==================================================\n",
      "Epoch Loss:  -0.8482923201152257\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "KNN Acc:  0.8555422168867547\n",
      "==================================================\n",
      "Epoch Loss:  -0.8494300331388202\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "KNN Acc:  0.85734293717487\n",
      "==================================================\n",
      "Epoch Loss:  -0.8501284956932068\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "KNN Acc:  0.8543417366946778\n",
      "==================================================\n",
      "Epoch Loss:  -0.8533885274614607\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "KNN Acc:  0.8531412565026011\n",
      "==================================================\n",
      "Epoch Loss:  -0.8509192483765738\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "KNN Acc:  0.8475390156062425\n",
      "==================================================\n",
      "Epoch Loss:  -0.8582413996968951\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "KNN Acc:  0.8391356542617047\n",
      "==================================================\n",
      "Epoch Loss:  -0.8689669506890433\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "KNN Acc:  0.8581432573029212\n",
      "==================================================\n",
      "Epoch Loss:  -0.8861454606056214\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "KNN Acc:  0.8435374149659864\n",
      "==================================================\n",
      "Epoch Loss:  -0.9095459529331752\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "KNN Acc:  0.8509403761504601\n",
      "==================================================\n",
      "Epoch Loss:  -0.9227312156132289\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "KNN Acc:  0.8463385354141657\n",
      "==================================================\n",
      "Epoch Loss:  -0.9308889746665955\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "KNN Acc:  0.8411364545818327\n",
      "==================================================\n",
      "Epoch Loss:  -0.9357749768665858\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "KNN Acc:  0.8533413365346139\n",
      "==================================================\n",
      "Epoch Loss:  -0.9399754694529943\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "KNN Acc:  0.8349339735894358\n",
      "==================================================\n",
      "Epoch Loss:  -0.9468883446284703\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "KNN Acc:  0.8407362945178071\n",
      "==================================================\n",
      "Epoch Loss:  -0.9516041125570025\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "KNN Acc:  0.8391356542617047\n",
      "==================================================\n",
      "Epoch Loss:  -0.9541059357779367\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "KNN Acc:  0.8389355742296919\n",
      "==================================================\n",
      "Epoch Loss:  -0.952596013886588\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "KNN Acc:  0.8399359743897559\n",
      "==================================================\n",
      "Epoch Loss:  -0.9531289713723319\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "KNN Acc:  0.8407362945178071\n",
      "==================================================\n",
      "Epoch Loss:  -0.9534960712705339\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "KNN Acc:  0.8411364545818327\n",
      "==================================================\n",
      "Epoch Loss:  -0.9847444500241961\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f2d0d4ae331b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_test_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mtest_embeds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_n_graphs_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mtest_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/libraries/torch_loc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/eslubana/graphssl/mpad/mpad/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj, n_graphs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_message_passing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/libraries/torch_loc/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/eslubana/graphssl/mpad/mpad/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_in, adj)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    LOAD TWO VIEWS\n",
    "    \"\"\"\n",
    "    adj_1,features_1,y_1 = load_epoch_txt(epoch_num=epoch,\n",
    "                                          l_enc=l_enc,\n",
    "                                          args=args)\n",
    "    adj_2,features_2,y_2 = load_epoch_txt(epoch_num=args.epochs -epoch,\n",
    "                                          l_enc=l_enc,\n",
    "                                          args=args)\n",
    "    \n",
    "    \n",
    "    #extract training samples!\n",
    "    adj_1 = [adj_1[i] for i in train_index]\n",
    "    features_1 = [features_1[i] for i in train_index]\n",
    "    y_1 = [y_1[i] for i in train_index]\n",
    "\n",
    "    adj_2 = [adj_2[i] for i in train_index]\n",
    "    features_2 = [features_2[i] for i in train_index]\n",
    "    y_2 = [y_2[i] for i in train_index]\n",
    "    \n",
    "    #generate batches\n",
    "    adj_1, features_1, batch_n_graphs_1, _ = generate_batches(adj_1,features_1, y_1, args.batch_size, args.use_master_node,shuffle=False)\n",
    "    adj_2, features_2, batch_n_graphs_2, _ = generate_batches(adj_2,features_2, y_2, args.batch_size, args.use_master_node,shuffle=False)\n",
    "\n",
    "   \n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Train for one epoch\n",
    "    for i in range(n_train_batches):\n",
    "        \n",
    "        loss = train_simsiam(sim,\n",
    "                             optimizer,\n",
    "                             epoch, \n",
    "                             adj_1[i],\n",
    "                             features_1[i],\n",
    "                             batch_n_graphs_1[i],\n",
    "                             adj_2[i],\n",
    "                             features_2[i])\n",
    "        running_loss += loss.item()\n",
    "    print(\"=\"*50)\n",
    "    print('Epoch {0} Loss: {1:.6f}'.format(epoch, running_loss/i))\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    sim.eval()\n",
    "    \n",
    "    \n",
    "    val_embeds = []\n",
    "    for i in range(n_val_batches):\n",
    "        with torch.no_grad():\n",
    "            val_embeds += sim.encoder[0](features_val[i], adj_val[i], batch_n_graphs_val[i])[1]\n",
    "    val_embeds = torch.stack(val_embeds).numpy()\n",
    "gmail.c\n",
    "    test_embeds = []\n",
    "    for i in range(n_test_batches):\n",
    "        with torch.no_grad():\n",
    "            test_embeds += sim.encoder[0](features_test[i], adj_test[i], batch_n_graphs_test[i])[1]\n",
    "    test_embeds = torch.stack(test_embeds).numpy()\n",
    "    \n",
    "    k = knn(val_embeds,torch.cat(y_val),test_embeds, torch.cat(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
