{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils import load_file, preprocessing, get_vocab, load_embeddings, create_gows, accuracy, generate_batches, AverageMeter, train,train_simsiam, load_R8, dotdict\n",
    "from models_v2 import MPAD, SimSiam\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['path_to_dataset'] = '../datasets/subjectivity.txt'\n",
    "args['path_to_embeddings'] = \"../GoogleNews-vectors-negative300.bin\"\n",
    "args['no_cuda'] = False\n",
    "args['epochs'] = 200\n",
    "args['lr'] = 0.001\n",
    "args['hidden'] = 64\n",
    "args['penultimate'] = 64\n",
    "args['message_passing_layers']=2\n",
    "args['window_size'] = 2\n",
    "args['directed'] = True\n",
    "args['use_master_node'] = True\n",
    "args['normalize'] = True\n",
    "args['dropout'] = 0.5\n",
    "args['batch_size'] = 128\n",
    "args['patience'] = 20\n",
    "args['rand_node_drop'] = 0.3\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\ttrain\tearn\n",
      "1\ttrain\tacq\n",
      "2\ttrain\tearn\n",
      "3\ttrain\tearn\n",
      "4\ttrain\tearn\n"
     ]
    }
   ],
   "source": [
    "! head -n5 ../datasets/R8_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.path_to_labelset = '../datasets/R8_labels.txt'\n",
    "#args.path_to_dataset = '../datasets/R8_data.txt'\n",
    "\n",
    "#docs, class_labels = load_R8(args.path_to_dataset,args.path_to_labelset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes:  2\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "docs, class_labels = load_file(args.path_to_dataset)\n",
    "docs = preprocessing(docs)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "class_labels = enc.fit_transform(class_labels)\n",
    "\n",
    "nclass = np.unique(class_labels).size\n",
    "y = list()\n",
    "for i in range(len(class_labels)):\n",
    "    t = np.zeros(1)\n",
    "    t[0] = class_labels[i]\n",
    "    y.append(t)\n",
    "print('Number of Classes: ',nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  21322\n",
      "Existing vectors: 17913\n"
     ]
    }
   ],
   "source": [
    "vocab = get_vocab(docs)\n",
    "embeddings = load_embeddings(\"../GoogleNews-vectors-negative300.bin\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, _ = create_gows(docs, vocab, args.window_size, args.directed, args.normalize, args.use_master_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Adj\" is a list of sparse tensors. \"Features\" is a list of np arrays where \"features[0]\" corresponds to a single graph, size = num nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "it = 0\n",
    "accs = list()\n",
    "train_index, test_index =  kf.split(y)\n",
    "train_index = train_index[0]\n",
    "test_index = test_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(train_index)\n",
    "train_index = idx[:int(idx.size*0.9)].tolist()\n",
    "val_index = idx[int(idx.size*0.9):].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  4500\n",
      "VAL:  500\n",
      "TEST:  5000\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_index)\n",
    "n_val = len(val_index)\n",
    "n_test = len(test_index)\n",
    "\n",
    "print(\"TRAIN: \",n_train)\n",
    "print(\"VAL: \",n_val)\n",
    "print(\"TEST: \",n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = [adj[i] for i in train_index]\n",
    "features_train = [features[i] for i in train_index]\n",
    "y_train = [y[i] for i in train_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_val = [adj[i] for i in val_index]\n",
    "features_val = [features[i] for i in val_index]\n",
    "y_val = [y[i] for i in val_index]\n",
    "adj_val, features_val, batch_n_graphs_val, y_val = generate_batches(adj_val, features_val, y_val, args.batch_size, args.use_master_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_test = [adj[i] for i in test_index]\n",
    "features_test = [features[i] for i in test_index]\n",
    "y_test = [y[i] for i in test_index]\n",
    "adj_test, features_test, batch_n_graphs_test, y_test = generate_batches(adj_test, features_test, y_test, args.batch_size, args.use_master_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_batches = ceil(n_train/args.batch_size)\n",
    "n_val_batcihes = ceil(n_val/args.batch_size)\n",
    "n_test_batches = ceil(n_test/args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using => GCN\n",
      "Using => GCN\n"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "model = MPAD(embeddings.shape[1], args.message_passing_layers, args.hidden, args.penultimate, nclass, args.dropout, embeddings, args.use_master_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    adj_train = [x.cuda() for x in adj_train]\n",
    "    features_train = [x.cuda() for x in features_train]\n",
    "    batch_n_graphs_train = [x.cuda() for x in batch_n_graphs_train]\n",
    "    y_train = [x.cuda() for x in y_train]\n",
    "    adj_val = [x.cuda() for x in adj_val]\n",
    "    features_val = [x.cuda() for x in features_val]\n",
    "    batch_n_graphs_val = [x.cuda() for x in batch_n_graphs_val]\n",
    "    y_val = [x.cuda() for x in y_val]\n",
    "    adj_test = [x.cuda() for x in adj_test]\n",
    "    features_test = [x.cuda() for x in features_test]\n",
    "    batch_n_graphs_test = [x.cuda() for x in batch_n_graphs_test]\n",
    "    y_test = [x.cuda() for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using => GCN\n",
      "Using => GCN\n"
     ]
    }
   ],
   "source": [
    "model = MPAD(embeddings.shape[1], \n",
    "             args.message_passing_layers, \n",
    "             args.hidden,\n",
    "             args.penultimate, \n",
    "             nclass, \n",
    "             args.dropout,\n",
    "             embeddings, \n",
    "             args.use_master_node)\n",
    "\n",
    "model.embedding_dim = 64\n",
    "\n",
    "sim = SimSiam(backbone=model,project_dim=64,bottle_neck_dim=10).to('cuda:0')\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, sim.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "args['lr'] = 0.001\n",
    "optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8, eta_min=1e-5,verbose=True)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5504, 5504])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_val[0].to_dense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ee67210d29cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "features_val[0].size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP:  0  --  torch.Size([5504, 64])\n",
      "MLP:  0  --  torch.Size([1, 5504, 64])\n",
      "MLP:  0  --  torch.Size([5504, 64])\n",
      "MLP:  0  --  torch.Size([1, 5504, 64])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8c41d5cc029e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mval_embeds\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_n_graphs_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mval_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#sim.eval()\n",
    "sim.train()\n",
    "val_embeds = []\n",
    "for i in range(3):\n",
    "    with torch.no_grad():\n",
    "        val_embeds += sim.encoder[0](features_val[i].to(\"cuda:0\"), adj_val[i].to_dense().unsqueeze(0).to(\"cuda:0\"), batch_n_graphs_val[i])[1]\n",
    "        break\n",
    "val_embeds = torch.stack(val_embeds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args['lr'] = 0.03\n",
    "# optimizer = optim.SGD(parameters, lr=args.lr)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=8, eta_min=1e-5,verbose=True)\n",
    "# #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPAD(\n",
       "  (embedding): Embedding(21323, 300)\n",
       "  (mps): ModuleList(\n",
       "    (0): MessagePassing(\n",
       "      (mlp1): MLP(\n",
       "        (linears): ModuleList(\n",
       "          (0): Linear(in_features=300, out_features=64, bias=True)\n",
       "          (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (batch_norms): ModuleList(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp2): MLP(\n",
       "        (linears): ModuleList(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (batch_norms): ModuleList(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (fc1_update): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2_update): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc1_reset): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2_reset): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): MessagePassing(\n",
       "      (mlp1): MLP(\n",
       "        (linears): ModuleList(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (batch_norms): ModuleList(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (mlp2): MLP(\n",
       "        (linears): ModuleList(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (batch_norms): ModuleList(\n",
       "          (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (fc1_update): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2_update): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc1_reset): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2_reset): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (atts): ModuleList(\n",
       "    (0): Attention(\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2): Linear(in_features=64, out_features=1, bias=False)\n",
       "      (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Attention(\n",
       "      (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (fc2): Linear(in_features=64, out_features=1, bias=False)\n",
       "      (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aug import rand_node_augment\n",
    "from utils import train_simsiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x_val,y_val,x_test,y_test):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=10)\n",
    "    neigh.fit(x_val, y_val)\n",
    "    score= neigh.score(x_test,y_test)\n",
    "    print(\"KNN Acc: \",score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sc/libraries/torch_loc/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.12607533665640014\n",
      "KNN Acc:  0.7884\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.36965705411774774\n",
      "KNN Acc:  0.8084\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.6018872397286551\n",
      "KNN Acc:  0.8196\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.6533943431718009\n",
      "KNN Acc:  0.8418\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.6794662662914821\n",
      "KNN Acc:  0.8508\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.6976972477776664\n",
      "KNN Acc:  0.846\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7095664518220084\n",
      "KNN Acc:  0.8432\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7114867755344936\n",
      "KNN Acc:  0.8438\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7122538294110979\n",
      "KNN Acc:  0.8428\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7166303958211626\n",
      "KNN Acc:  0.8442\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7448313610894339\n",
      "KNN Acc:  0.8344\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.7727252892085484\n",
      "KNN Acc:  0.8402\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.82886130128588\n",
      "KNN Acc:  0.8444\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.8559255634035383\n",
      "KNN Acc:  0.8312\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.8679634775434222\n",
      "KNN Acc:  0.8342\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "==================================================\n",
      "Epoch Loss:  -0.8768326810428074\n",
      "KNN Acc:  0.8272\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.8976786954062326\n",
      "KNN Acc:  0.8214\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9107617616653443\n",
      "KNN Acc:  0.8184\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9245843631880624\n",
      "KNN Acc:  0.8174\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9389332720211574\n",
      "KNN Acc:  0.8168\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9486599768911089\n",
      "KNN Acc:  0.8154\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9532414742878506\n",
      "KNN Acc:  0.8138\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9544169630323137\n",
      "KNN Acc:  0.8122\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9539712377956935\n",
      "KNN Acc:  0.8124\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9563038434301104\n",
      "KNN Acc:  0.8108\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9582827636173793\n",
      "KNN Acc:  0.8112\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9638641612870352\n",
      "KNN Acc:  0.8128\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9674449171338763\n",
      "KNN Acc:  0.8092\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9738755924361092\n",
      "KNN Acc:  0.8168\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9802611214773995\n",
      "KNN Acc:  0.8154\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9874418224607195\n",
      "KNN Acc:  0.8096\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9919857825551714\n",
      "KNN Acc:  0.8004\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9941518579210554\n",
      "KNN Acc:  0.7968\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9968745010239738\n",
      "KNN Acc:  0.8026\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.999090143612453\n",
      "KNN Acc:  0.8018\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9993462783949716\n",
      "KNN Acc:  0.7982\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.000154161453247\n",
      "KNN Acc:  0.8028\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "KNN Acc:  0.7998\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.002487885951996\n",
      "KNN Acc:  0.7918\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0018338901656014\n",
      "KNN Acc:  0.793\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0010509610176086\n",
      "KNN Acc:  0.7952\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0012503862380981\n",
      "KNN Acc:  0.788\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0012828213827951\n",
      "KNN Acc:  0.8032\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0010379246303014\n",
      "KNN Acc:  0.8152\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -0.9993149535996574\n",
      "KNN Acc:  0.8132\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0028178879192897\n",
      "KNN Acc:  0.816\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0058821422713144\n",
      "KNN Acc:  0.8126\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.004638147354126\n",
      "KNN Acc:  0.8198\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0046969073159353\n",
      "KNN Acc:  0.8282\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0046894822801862\n",
      "KNN Acc:  0.8226\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0055165920938764\n",
      "KNN Acc:  0.823\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0039987683296203\n",
      "KNN Acc:  0.8236\n",
      "Adjusting learning rate of group 0 to 4.7680e-05.\n",
      "==================================================\n",
      "Epoch Loss:  -1.00467643226896\n",
      "KNN Acc:  0.8256\n",
      "Adjusting learning rate of group 0 to 1.5498e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0052671807152884\n",
      "KNN Acc:  0.8278\n",
      "Adjusting learning rate of group 0 to 3.1557e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0058789593832833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Acc:  0.8176\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.003283817427499\n",
      "KNN Acc:  0.827\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0034732443945749\n",
      "KNN Acc:  0.8298\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.00400596005576\n",
      "KNN Acc:  0.828\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.003084225314004\n",
      "KNN Acc:  0.8402\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "==================================================\n",
      "Epoch Loss:  -1.003078726359776\n",
      "KNN Acc:  0.8404\n",
      "Adjusting learning rate of group 0 to 9.6232e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.0050073300089155\n",
      "KNN Acc:  0.8386\n",
      "Adjusting learning rate of group 0 to 8.5502e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.00602194581713\n",
      "KNN Acc:  0.8252\n",
      "Adjusting learning rate of group 0 to 6.9443e-04.\n",
      "==================================================\n",
      "Epoch Loss:  -1.008393018586295\n",
      "KNN Acc:  0.825\n",
      "Adjusting learning rate of group 0 to 5.0500e-04.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-473621ddafde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0madj_train_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_node_augment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0madj_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_n_graphs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_train_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_master_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sc/eslubana/graphssl/mpad/mpad/aug.py\u001b[0m in \u001b[0;36mrand_node_augment\u001b[0;34m(adj, args)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclipping_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_node_drop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mr_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclipping_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtmp_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0ma_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtmp_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0ma_prime\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "adj_train_1, features_train_1, batch_n_graphs_train_1, y_train_1 = generate_batches(adj_train, \n",
    "                                                                            features_train,\n",
    "                                                                            y_train, \n",
    "                                                                            args.batch_size,\n",
    "                                                                            args.use_master_node)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    adj_train_2 = rand_node_augment(adj_train, args)\n",
    "    adj_train_2, features_train_2, batch_n_graphs_train, _ = generate_batches(adj_train_2,features_train, y_train, args.batch_size, args.use_master_node)\n",
    "\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    running_loss = 0.0\n",
    "    # Train for one epoch\n",
    "    for i in range(n_train_batches):\n",
    "        \n",
    "        loss = train_simsiam(sim,\n",
    "                             optimizer,\n",
    "                             epoch, \n",
    "                             adj_train_1[i],\n",
    "                             features_train_1[i],\n",
    "                             batch_n_graphs_train[i],\n",
    "                             adj_train_2[i],\n",
    "                             features_train_2[i],\n",
    "                            )\n",
    "        train_loss.update(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        #train_acc.update(accuracy(output.data, y_train[i].data), output.size(0))\n",
    "        #print(loss.item())\n",
    "    print(\"=\"*50)\n",
    "    print('Epoch Loss: ', running_loss/i)\n",
    "    sim.eval()\n",
    "\n",
    "    val_embeds = []\n",
    "    for i in range(n_val_batches):\n",
    "        with torch.no_grad():\n",
    "            val_embeds += sim.encoder[0](features_val[i], adj_val[i], batch_n_graphs_val[i])[1]\n",
    "    val_embeds = torch.stack(val_embeds).numpy()\n",
    "\n",
    "    test_embeds = []\n",
    "    for i in range(n_test_batches):\n",
    "        with torch.no_grad():\n",
    "            test_embeds += sim.encoder[0](features_test[i], adj_test[i], batch_n_graphs_test[i])[1]\n",
    "    test_embeds = torch.stack(test_embeds).numpy()\n",
    "    \n",
    "    k = knn(val_embeds,torch.cat(y_val),test_embeds, torch.cat(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e67072f8a08b9d4e216dcf0eadfd85f060c3ca261535a0b59f6c20ecad76113b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}